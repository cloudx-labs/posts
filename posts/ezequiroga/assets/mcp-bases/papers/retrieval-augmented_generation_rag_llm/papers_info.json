{
  "2409.01666v1": {
    "title": "In Defense of RAG in the Era of Long-Context Language Models",
    "authors": [
      "Tan Yu",
      "Anbang Xu",
      "Rama Akkiraju"
    ],
    "summary": "Overcoming the limited context limitations in early-generation LLMs,\nretrieval-augmented generation (RAG) has been a reliable solution for\ncontext-based answer generation in the past. Recently, the emergence of\nlong-context LLMs allows the models to incorporate much longer text sequences,\nmaking RAG less attractive. Recent studies show that long-context LLMs\nsignificantly outperform RAG in long-context applications. Unlike the existing\nworks favoring the long-context LLM over RAG, we argue that the extremely long\ncontext in LLMs suffers from a diminished focus on relevant information and\nleads to potential degradation in answer quality. This paper revisits the RAG\nin long-context answer generation. We propose an order-preserve\nretrieval-augmented generation (OP-RAG) mechanism, which significantly improves\nthe performance of RAG for long-context question-answer applications. With\nOP-RAG, as the number of retrieved chunks increases, the answer quality\ninitially rises, and then declines, forming an inverted U-shaped curve. There\nexist sweet points where OP-RAG could achieve higher answer quality with much\nless tokens than long-context LLM taking the whole context as input. Extensive\nexperiments on public benchmark demonstrate the superiority of our OP-RAG.",
    "pdf_url": "http://arxiv.org/pdf/2409.01666v1",
    "published": "2024-09-03"
  },
  "2501.00353v1": {
    "title": "RAG-Instruct: Boosting LLMs with Diverse Retrieval-Augmented Instructions",
    "authors": [
      "Wanlong Liu",
      "Junying Chen",
      "Ke Ji",
      "Li Zhou",
      "Wenyu Chen",
      "Benyou Wang"
    ],
    "summary": "Retrieval-Augmented Generation (RAG) has emerged as a key paradigm for\nenhancing large language models (LLMs) by incorporating external knowledge.\nHowever, current RAG methods face two limitations: (1) they only cover limited\nRAG scenarios. (2) They suffer from limited task diversity due to the lack of a\ngeneral RAG dataset. To address these limitations, we propose RAG-Instruct, a\ngeneral method for synthesizing diverse and high-quality RAG instruction data\nbased on any source corpus. Our approach leverages (1) five RAG paradigms,\nwhich encompass diverse query-document relationships, and (2) instruction\nsimulation, which enhances instruction diversity and quality by utilizing the\nstrengths of existing instruction datasets. Using this method, we construct a\n40K instruction dataset from Wikipedia, comprehensively covering diverse RAG\nscenarios and tasks. Experiments demonstrate that RAG-Instruct effectively\nenhances LLMs' RAG capabilities, achieving strong zero-shot performance and\nsignificantly outperforming various RAG baselines across a diverse set of\ntasks. RAG-Instruct is publicly available at\nhttps://github.com/FreedomIntelligence/RAG-Instruct.",
    "pdf_url": "http://arxiv.org/pdf/2501.00353v1",
    "published": "2024-12-31"
  },
  "2407.21059v1": {
    "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
    "authors": [
      "Yunfan Gao",
      "Yun Xiong",
      "Meng Wang",
      "Haofen Wang"
    ],
    "summary": "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\nof Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\nincreasing demands of application scenarios have driven the evolution of RAG,\nleading to the integration of advanced retrievers, LLMs and other complementary\ntechnologies, which in turn has amplified the intricacy of RAG systems.\nHowever, the rapid advancements are outpacing the foundational RAG paradigm,\nwith many methods struggling to be unified under the process of\n\"retrieve-then-generate\". In this context, this paper examines the limitations\nof the existing RAG paradigm and introduces the modular RAG framework. By\ndecomposing complex RAG systems into independent modules and specialized\noperators, it facilitates a highly reconfigurable framework. Modular RAG\ntranscends the traditional linear architecture, embracing a more advanced\ndesign that integrates routing, scheduling, and fusion mechanisms. Drawing on\nextensive research, this paper further identifies prevalent RAG\npatterns-linear, conditional, branching, and looping-and offers a comprehensive\nanalysis of their respective implementation nuances. Modular RAG presents\ninnovative opportunities for the conceptualization and deployment of RAG\nsystems. Finally, the paper explores the potential emergence of new operators\nand paradigms, establishing a solid theoretical foundation and a practical\nroadmap for the continued evolution and practical deployment of RAG\ntechnologies.",
    "pdf_url": "http://arxiv.org/pdf/2407.21059v1",
    "published": "2024-07-26"
  },
  "2501.05249v1": {
    "title": "RAG-WM: An Efficient Black-Box Watermarking Approach for Retrieval-Augmented Generation of Large Language Models",
    "authors": [
      "Peizhuo Lv",
      "Mengjie Sun",
      "Hao Wang",
      "Xiaofeng Wang",
      "Shengzhi Zhang",
      "Yuxuan Chen",
      "Kai Chen",
      "Limin Sun"
    ],
    "summary": "In recent years, tremendous success has been witnessed in Retrieval-Augmented\nGeneration (RAG), widely used to enhance Large Language Models (LLMs) in\ndomain-specific, knowledge-intensive, and privacy-sensitive tasks. However,\nattackers may steal those valuable RAGs and deploy or commercialize them,\nmaking it essential to detect Intellectual Property (IP) infringement. Most\nexisting ownership protection solutions, such as watermarks, are designed for\nrelational databases and texts. They cannot be directly applied to RAGs because\nrelational database watermarks require white-box access to detect IP\ninfringement, which is unrealistic for the knowledge base in RAGs. Meanwhile,\npost-processing by the adversary's deployed LLMs typically destructs text\nwatermark information. To address those problems, we propose a novel black-box\n\"knowledge watermark\" approach, named RAG-WM, to detect IP infringement of\nRAGs. RAG-WM uses a multi-LLM interaction framework, comprising a Watermark\nGenerator, Shadow LLM & RAG, and Watermark Discriminator, to create watermark\ntexts based on watermark entity-relationship tuples and inject them into the\ntarget RAG. We evaluate RAG-WM across three domain-specific and two\nprivacy-sensitive tasks on four benchmark LLMs. Experimental results show that\nRAG-WM effectively detects the stolen RAGs in various deployed LLMs.\nFurthermore, RAG-WM is robust against paraphrasing, unrelated content removal,\nknowledge insertion, and knowledge expansion attacks. Lastly, RAG-WM can also\nevade watermark detection approaches, highlighting its promising application in\ndetecting IP infringement of RAG systems.",
    "pdf_url": "http://arxiv.org/pdf/2501.05249v1",
    "published": "2025-01-09"
  },
  "2504.08758v1": {
    "title": "Hyper-RAG: Combating LLM Hallucinations using Hypergraph-Driven Retrieval-Augmented Generation",
    "authors": [
      "Yifan Feng",
      "Hao Hu",
      "Xingliang Hou",
      "Shiquan Liu",
      "Shihui Ying",
      "Shaoyi Du",
      "Han Hu",
      "Yue Gao"
    ],
    "summary": "Large language models (LLMs) have transformed various sectors, including\neducation, finance, and medicine, by enhancing content generation and\ndecision-making processes. However, their integration into the medical field is\ncautious due to hallucinations, instances where generated content deviates from\nfactual accuracy, potentially leading to adverse outcomes. To address this, we\nintroduce Hyper-RAG, a hypergraph-driven Retrieval-Augmented Generation method\nthat comprehensively captures both pairwise and beyond-pairwise correlations in\ndomain-specific knowledge, thereby mitigating hallucinations. Experiments on\nthe NeurologyCrop dataset with six prominent LLMs demonstrated that Hyper-RAG\nimproves accuracy by an average of 12.3% over direct LLM use and outperforms\nGraph RAG and Light RAG by 6.3% and 6.0%, respectively. Additionally, Hyper-RAG\nmaintained stable performance with increasing query complexity, unlike existing\nmethods which declined. Further validation across nine diverse datasets showed\na 35.5% performance improvement over Light RAG using a selection-based\nassessment. The lightweight variant, Hyper-RAG-Lite, achieved twice the\nretrieval speed and a 3.3% performance boost compared with Light RAG. These\nresults confirm Hyper-RAG's effectiveness in enhancing LLM reliability and\nreducing hallucinations, making it a robust solution for high-stakes\napplications like medical diagnostics.",
    "pdf_url": "http://arxiv.org/pdf/2504.08758v1",
    "published": "2025-03-30"
  }
}